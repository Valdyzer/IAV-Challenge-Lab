{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473533c8-b43b-4a60-aaee-7f63ee2d8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements for python, torch and companion libraries and your own modules\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "### torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45efaf-25b9-4d8c-ae7b-65b2dff25e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables defining training hyper-parameters among other things \n",
    "model_kwargs={\n",
    "                                'embed_dim': 256,\n",
    "                                'hidden_dim': 512,\n",
    "                                'num_heads': 8,\n",
    "                                'num_layers': 6,\n",
    "                                'patch_size': 4,\n",
    "                                'num_channels': 3,\n",
    "                                'num_patches': 64,\n",
    "                                'num_classes': 80,\n",
    "                                'dropout': 0.2\n",
    "                            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74c4e0-d696-489b-9d40-5d07430539bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device initialization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaffe622-4208-4251-bf1f-eeb8580a12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directories initialization\n",
    "class COCOTrainImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, annotations_dir, max_images=None, transform=None):\n",
    "        self.img_labels = sorted(glob(\"*.cls\", root_dir=annotations_dir))\n",
    "        if max_images:\n",
    "            self.img_labels = self.img_labels[:max_images]\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, Path(self.img_labels[idx]).stem + \".jpg\")\n",
    "        labels_path = os.path.join(self.annotations_dir, self.img_labels[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(labels_path) as f: \n",
    "            labels = [int(label) for label in f.readlines()]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        labels = torch.zeros(80).scatter_(0, torch.tensor(labels), value=1)\n",
    "        return image, labels\n",
    "\n",
    "\n",
    "class COCOTestImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_list = sorted(glob(\"*.jpg\", root_dir=img_dir))    \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, Path(img_path).stem # filename w/o extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873cfc8-7fff-4402-9677-b25b330d3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiation of transforms, datasets and data loaders\n",
    "# TIP : use torch.utils.data.random_split to split the training set into train and validation subsets\n",
    "\n",
    "# train_img : val_img = 9:1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd97a12-7d65-416d-94a0-2d17971382da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class definitions\n",
    "classes = (\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \n",
    "           \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
    "           \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",       \n",
    "           \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "           \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "           \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n",
    "           \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \n",
    "           \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \n",
    "           \"hair drier\", \"toothbrush\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0d9c4-42a9-42be-9549-942e427498dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiation and preparation of network model\n",
    "\n",
    "# swintransformer V2 small\n",
    "from torchvision.models import swin_v2_s,Swin_V2_S_Weights\n",
    "\n",
    "model = swin_v2_s(weights = Swin_V2_S_Weights.DEFAULT)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(?, 80)\n",
    "\n",
    "import torchvision.transforms as transforms \n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                # other transforms,\n",
    "                                Swin_V2_S_Weights.DEFAULT.transforms()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efeadbd-0684-46fc-b009-ec06728d8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiation of loss criterion\n",
    "# instantiation of optimizer, registration of network parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f241fce-0cc1-47d2-90e3-6fe9fd7d6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of current best model path\n",
    "# initialization of model selection metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6474e0-2227-4402-8d9e-89854a87aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of tensorboard SummaryWriter (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2850d7-68f5-4a4a-affa-9d04c27b536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs loop:\n",
    "#   train\n",
    "#   validate on train set\n",
    "#   validate on validation set\n",
    "#   update graphs (optional)\n",
    "#   is new model better than current model ?\n",
    "#       save it, update current best metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522321e-c0a4-4b2b-8d73-59af72ed06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close tensorboard SummaryWriter if created (optional)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
