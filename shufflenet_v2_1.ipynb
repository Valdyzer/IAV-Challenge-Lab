{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a4a768",
   "metadata": {},
   "source": [
    "## Library Imports and Environment Setup\n",
    "\n",
    "**Design Rationale**: Separating dataset classes into a standalone Python module resolves Windows-specific pickling issues when using `num_workers > 0` in DataLoader, enabling parallel data loading for faster training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9094f777-f1c1-4224-b248-3a044861fca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# import statements for python, torch and companion libraries and your own modules\n",
    "import os\n",
    "import sys\n",
    "#nb_dir = os.path.split(os.getcwd())[0]\n",
    "#if nb_dir not in sys.path:\n",
    "    #sys.path.append(nb_dir)\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "#from lion_pytorch import Lion\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import shufflenet_v2_x1_0, ShuffleNet_V2_X1_0_Weights\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Import dataset classes from dataset.py for Windows multiprocessing support\n",
    "from dataset import COCOTrainImageDataset, COCOTestImageDataset, ValidationDataset\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177cf30",
   "metadata": {},
   "source": [
    "## Reproducibility Configuration\n",
    "\n",
    "**Technical Details**:\n",
    "- **Fixed random seeds (42)**: Ensures identical weight initialization and data shuffling across experiments for reproducible results\n",
    "- **cuDNN settings**:\n",
    "  - `deterministic=False`: Prioritizes computational performance, as cuDNN can select fastest algorithms\n",
    "  - `benchmark=True`: Enables cuDNN autotuner to benchmark and select optimal convolution algorithms for the fixed input size, enable for speedingup\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7286f114-09ea-4ea1-88d3-362671cb0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = False  \n",
    "    torch.backends.cudnn.benchmark = True  \n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd44f2",
   "metadata": {},
   "source": [
    "## Hyperparameter Configuration\n",
    "\n",
    "**Key Hyperparameters**:\n",
    "- **Batch size (128)**: Larger batch size enables more stable gradient estimates and better GPU utilization, allowing for slightly higher learning rates through linear scaling rule\n",
    "- **Learning rate (4e-4)**: Scaled from base 3e-4 for efficient convergence within 20 epochs, suitable for fine-tuning the lightweight ShuffleNet architecture on COCO\n",
    "- **Weight decay (5e-5)**: Moderate L2 regularization strength provides effective generalization on 65K training samples without over-constraining model capacity\n",
    "\n",
    "**Model Selection Metric**:\n",
    "- **Validation Loss**: Direct optimization target that provides stable and reliable checkpointing, ensuring the saved model represents the best generalization performance\n",
    "- **Micro F1**:\n",
    "- **Macro F1**:\n",
    "- **mAP**:\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d39e4257-d696-4a4b-ba57-dfb209dba058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global variables and hyperparameters defined:\n",
      "  - Batch size: 128\n",
      "  - Number of epochs: 20\n",
      "  - Learning rate: 0.0003\n",
      "  - Validation split: 0.1\n",
      "  - Threshold: 0.5\n",
      "  - Model selection metric: val_loss\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# global variables defining training hyper-parameters among other things \n",
    "BATCH_SIZE = 128  \n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 3e-4  \n",
    "WEIGHT_DECAY = 5e-5\n",
    "NUM_CLASSES = 80\n",
    "VALIDATION_SPLIT = 0.1\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "# Options: 'val_loss', 'micro_f1', 'macro_f1', 'mAP'\n",
    "METRIC_OPTION = 'val_loss'  \n",
    "\n",
    "print(\"Global variables and hyperparameters defined:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Number of epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Validation split: {VALIDATION_SPLIT}\")\n",
    "print(f\"  - Threshold: {THRESHOLD}\")\n",
    "print(f\"  - Model selection metric: {METRIC_OPTION}\")\n",
    "\n",
    "# device initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b31cd1d-5acf-4cd8-8dde-49409a28d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directories initialization\n",
    "DATA_DIR = \"ms-coco\"\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"images\", \"train-resized\", \"train-resized\")\n",
    "TEST_IMG_DIR = os.path.join(DATA_DIR, \"images\", \"test-resized\", \"test-resized\")\n",
    "TRAIN_LABELS_DIR = os.path.join(DATA_DIR, \"labels\", \"train\")\n",
    "MODEL_SAVE_PATH = \"best_coco_shuffle_model.pth\"\n",
    "OUTPUT_JSON_FILE = \"coco_predictions_shuffle_v9.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d64c5467-88d4-4ab5-b910-9a8d9650fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class definitions\n",
    "classes = (\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \n",
    "           \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
    "           \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",       \n",
    "           \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "           \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "           \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n",
    "           \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \n",
    "           \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \n",
    "           \"hair drier\", \"toothbrush\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "377a65a4-12c8-4855-bc63-32c78e291553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories and class names defined:\n",
      "  - Training images: ms-coco\\images\\train-resized\\train-resized\n",
      "  - Test images: ms-coco\\images\\test-resized\\test-resized\n",
      "  - Training labels: ms-coco\\labels\\train\n",
      "  - Dataset contains 80 classes\n"
     ]
    }
   ],
   "source": [
    "print(\"Data directories and class names defined:\")\n",
    "print(f\"  - Training images: {TRAIN_IMG_DIR}\")\n",
    "print(f\"  - Test images: {TEST_IMG_DIR}\")\n",
    "print(f\"  - Training labels: {TRAIN_LABELS_DIR}\")\n",
    "print(f\"  - Dataset contains {NUM_CLASSES} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca2d4de6-7658-42e9-bdc3-6d27c95aadc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Full training dataset size: 65000\n",
      "Training set size: 58500\n",
      "Validation set size: 6500\n"
     ]
    }
   ],
   "source": [
    "# instantiation of transforms, datasets and data loaders\n",
    "# TIP : use torch.utils.data.random_split to split the training set into train and validation subsets\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),   \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create full training dataset\n",
    "print(\"Loading dataset...\")\n",
    "full_train_dataset = COCOTrainImageDataset(\n",
    "    img_dir=TRAIN_IMG_DIR,\n",
    "    annotations_dir=TRAIN_LABELS_DIR,\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "print(f\"Full training dataset size: {len(full_train_dataset)}\")\n",
    "\n",
    "# Split training data into train and validation subsets using torch.utils.data.random_split\n",
    "train_size = int((1 - VALIDATION_SPLIT) * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124122de",
   "metadata": {},
   "source": [
    "## Data Augmentation and Dataset Initialization\n",
    "\n",
    "**Training Augmentations**:\n",
    "- **Resize to 224×224**: Matches ShuffleNet V2 input requirements\n",
    "- **BILINEAR interpolation**: Smooth resampling preserving edge details better than nearest-neighbor\n",
    "- **RandomHorizontalFlip (p=0.5)**: Introduces horizontal symmetry as data augmentation, effective for object-centric datasets like COCO where orientation variance exists\n",
    "- **ImageNet normalization**: Uses standard mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] to match pre-training statistics, crucial for transfer learning\n",
    "\n",
    "**Validation Transformations**:\n",
    "- **No augmentation**: Only resize and normalize to evaluate model on clean data\n",
    "- Ensures unbiased performance estimation\n",
    "\n",
    "**Rationale**: Minimal augmentation strategy reduces training time while preserving sample diversity. More aggressive augmentation (rotation, color jitter) was avoided to maintain COCO's naturalistic image characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c0b5c2-fe6a-4ca1-9973-bbc1d18351cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created successfully with Windows multiprocessing support\n",
      "  - Training loader: 457 batches, 6 workers\n",
      "  - Validation loader: 51 batches, 6 workers\n"
     ]
    }
   ],
   "source": [
    "val_dataset_transformed = ValidationDataset(val_dataset, val_transforms)\n",
    "\n",
    "# Create data loaders with Windows-compatible multiprocessing settings\n",
    "# For Windows, we can now use num_workers > 0 since dataset classes are in separate .py file\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=6,  \n",
    "    pin_memory=True,  \n",
    "    drop_last=True,\n",
    "    persistent_workers=True  # Keep workers alive between epochs\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_transformed, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=6,  \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(\"Data loaders created successfully with Windows multiprocessing support\")\n",
    "print(f\"  - Training loader: {len(train_loader)} batches, {train_loader.num_workers} workers\")\n",
    "print(f\"  - Validation loader: {len(val_loader)} batches, {val_loader.num_workers} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d622796",
   "metadata": {},
   "source": [
    "## Model Architecture Definition\n",
    "\n",
    "**Architecture Rationale**:\n",
    "\n",
    "**Backbone Choice - ShuffleNet V2 x1.0**:\n",
    "- **Efficiency-oriented**: Designed for mobile/edge devices with only ~2.3M parameters, enabling fast training and inference\n",
    "- **Pre-trained on ImageNet**: Provides strong initial feature extractors for transfer learning\n",
    "- **COCO training experience**: ShuffleNet has been successfully trained on COCO dataset in prior work, demonstrating its effectiveness for multi-label object classification tasks\n",
    "- **Compact model size**: The relatively small parameter count (~1.8M) allows for more transparent observation of how each fine-tuning operation affects model performance, making it ideal for experimental analysis and hyperparameter tuning\n",
    "\n",
    "**Classification Head Design**:\n",
    "- **Dropout layers (0.3, 0.2)**: Stochastic regularization with graduated dropout rates - higher in first layer where features are more task-specific, lower before final classification\n",
    "- **Intermediate 512-dim layer**: Provides sufficient capacity for learning complex multi-label patterns while maintaining parameter efficiency\n",
    "- **ReLU activation**: Standard non-linearity for intermediate representations, enabling effective gradient flow\n",
    "- **Output dimension = 80**: One logit per COCO class for independent multi-label prediction\n",
    "\n",
    "**Multi-Label Formulation**: Unlike single-label classification, no softmax is applied - instead, sigmoid activation (applied later) treats each class independently, enabling multiple simultaneous predictions per image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c22b8d27-dafe-4770-8c34-603330f9df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int = 80, pretrained: bool = True):\n",
    "        super(COCOMultiLabelClassifier, self).__init__()\n",
    "        \n",
    "        # Use pre-trained ShuffleNet V2 x1.0 as backbone\n",
    "        if pretrained:\n",
    "            self.backbone = shufflenet_v2_x1_0(weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            self.backbone = shufflenet_v2_x1_0(weights=None)\n",
    "        \n",
    "        # ShuffleNet V2 x1.0 has 1024 output features\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Replace classification head with multi-label classification head\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4cefe",
   "metadata": {},
   "source": [
    "## DataLoader Configuration with Multiprocessing\n",
    "\n",
    "**Optimization Strategies**:\n",
    "- **num_workers=6**: Spawns 6 background processes for asynchronous data loading, reducing GPU idle time\n",
    "- **pin_memory=True**: Allocates tensors in page-locked memory for faster CPU→GPU transfer via DMA\n",
    "- **persistent_workers=True**: Keeps worker processes alive between epochs, avoiding spawn overhead (significant on Windows)\n",
    "- **drop_last=True** (train only): Ensures consistent batch sizes, preventing batch normalization issues with small final batches\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d09d758-7c30-4df4-a2b4-e77c429d372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Model loaded to device: cuda\n",
      "  - Total parameters: 1,819,444\n",
      "  - Trainable parameters: 1,819,444\n"
     ]
    }
   ],
   "source": [
    "# instantiation and preparation of network model\n",
    "print(\"Initializing model...\")\n",
    "model = COCOMultiLabelClassifier(num_classes=NUM_CLASSES, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded to device: {device}\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22e1cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mAP(predictions, labels):\n",
    "    predictions_np = predictions.cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    \n",
    "    aps = []\n",
    "    for class_idx in range(labels.shape[1]):\n",
    "        y_true = labels_np[:, class_idx]\n",
    "        y_scores = predictions_np[:, class_idx]\n",
    "        \n",
    "        # Skip classes with no positive samples\n",
    "        if y_true.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Sort by prediction scores (descending)\n",
    "        sorted_indices = np.argsort(-y_scores)\n",
    "        y_true_sorted = y_true[sorted_indices]\n",
    "        \n",
    "        # Calculate precision at each threshold\n",
    "        tp = np.cumsum(y_true_sorted)\n",
    "        fp = np.cumsum(1 - y_true_sorted)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        \n",
    "        total_positives = y_true.sum()\n",
    "        recall = tp / total_positives\n",
    "        \n",
    "        precision = np.concatenate([[0], precision, [0]])\n",
    "        recall = np.concatenate([[0], recall, [1]])\n",
    "        \n",
    "        for i in range(len(precision) - 2, -1, -1):\n",
    "            precision[i] = max(precision[i], precision[i + 1])\n",
    "        \n",
    "        ap = np.sum((recall[1:] - recall[:-1]) * precision[1:])\n",
    "        aps.append(ap)\n",
    "    \n",
    "    if len(aps) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    mAP = np.mean(aps)\n",
    "    return float(mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e22549af-61ff-4797-9526-0e8851ba54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for select the best model\n",
    "def calculate_f1_metrics(predictions, labels, threshold=0.5):\n",
    "\n",
    "    predictions_binary = (predictions > threshold).float()\n",
    "    tp = (predictions_binary * labels).sum()\n",
    "    fp = (predictions_binary * (1 - labels)).sum() \n",
    "    fn = ((1 - predictions_binary) * labels).sum()\n",
    "    \n",
    "    micro_precision = tp / (tp + fp + 1e-8)\n",
    "    micro_recall = tp / (tp + fn + 1e-8)\n",
    "    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall + 1e-8)\n",
    "    \n",
    "    class_f1s = []\n",
    "    for c in range(labels.shape[1]):\n",
    "        tp_c = (predictions_binary[:, c] * labels[:, c]).sum()\n",
    "        fp_c = (predictions_binary[:, c] * (1 - labels[:, c])).sum()\n",
    "        fn_c = ((1 - predictions_binary[:, c]) * labels[:, c]).sum()\n",
    "        \n",
    "        prec_c = tp_c / (tp_c + fp_c + 1e-8)\n",
    "        rec_c = tp_c / (tp_c + fn_c + 1e-8)\n",
    "        f1_c = 2 * prec_c * rec_c / (prec_c + rec_c + 1e-8)\n",
    "        class_f1s.append(f1_c)\n",
    "    \n",
    "    macro_f1 = torch.stack(class_f1s).mean()\n",
    "    return float(micro_f1), float(macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d1bf3d5-fb71-4fb7-8610-8167335f49de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader: DataLoader, net: nn.Module, criterion: nn.Module, \n",
    "               optimizer: optim.Optimizer, device: torch.device) -> float:\n",
    "\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\",position=0, leave=True):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67e949",
   "metadata": {},
   "source": [
    "## Mean Average Precision (mAP) Metric Implementation\n",
    "\n",
    "**Metric Rationale**:\n",
    "- **mAP superiority over F1**: Evaluates ranking quality across all thresholds, not just a single operating point\n",
    "- **Per-class AP calculation**: Measures precision-recall area for each class independently\n",
    "- **Handles class imbalance**: Averaging per-class APs gives equal weight to rare and common classes\n",
    "\n",
    "**Algorithm Details**:\n",
    "1. **Per-class processing**: Iterate through all 80 classes\n",
    "2. **Sort by confidence**: Rank predictions by sigmoid probabilities (descending)\n",
    "3. **Precision-recall curve**: Compute cumulative TP/FP at each threshold\n",
    "4. **Monotonic interpolation**: Ensure precision is non-increasing with recall (standard VOC/COCO protocol)\n",
    "5. **Area under curve**: Integrate using trapezoidal rule\n",
    "6. **Skip empty classes**: Excludes classes with no positive samples from averaging\n",
    "\n",
    "**Technical Considerations**:\n",
    "- **Numerical stability**: Added epsilon (1e-8) prevents division by zero\n",
    "- **Boundary conditions**: Extends curve with (0,0) and (1,0) endpoints for proper area calculation\n",
    "- **COCO alignment**: Implementation follows official COCO evaluation protocol for fair benchmark comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "566f95e5-0e55-482d-91a7-2c97680cf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(val_loader: DataLoader, net: nn.Module, criterion: nn.Module, \n",
    "                   device: torch.device) -> Dict[str, float]:\n",
    "\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\",position=0, leave=True):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            batch_loss = criterion(outputs, labels)\n",
    "            val_loss += batch_loss.item() * images.size(0)\n",
    "            \n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            \n",
    "            all_predictions.append(probabilities.cpu())  # save the probabilities instead of predictions\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    micro_f1, macro_f1 = calculate_f1_metrics(all_predictions, all_labels)\n",
    "    mAP = calculate_mAP(all_predictions, all_labels)\n",
    "\n",
    "    predictions_binary = (all_predictions > THRESHOLD).float()\n",
    "    exact_match = (all_predictions == all_labels).all(dim=1).float().mean().item()\n",
    "    \n",
    "    sample_accuracy = ((all_predictions == all_labels).float().mean(dim=1)).mean().item()\n",
    "    \n",
    "    return {\n",
    "        'loss': val_loss,\n",
    "        'exact_match_accuracy': exact_match,\n",
    "        'sample_accuracy': sample_accuracy,\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'mAP': mAP,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d38dc",
   "metadata": {},
   "source": [
    "## F1 Score Metrics (Micro and Macro)\n",
    "\n",
    "**Metric Definitions**:\n",
    "\n",
    "**Micro F1**:\n",
    "- **Global aggregation**: Computes precision/recall from aggregated TP/FP/FN across all classes\n",
    "- **Interpretation**: Overall performance weighted by class frequency\n",
    "- **Bias**: Favors common classes in imbalanced datasets\n",
    "\n",
    "**Macro F1**:\n",
    "- **Per-class averaging**: Computes F1 for each class independently, then averages\n",
    "- **Interpretation**: Treats all classes equally regardless of frequency\n",
    "- **Bias**: Better reflects performance on rare classes\n",
    "\n",
    "**Implementation Details**:\n",
    "- **Threshold application**: Converts probabilities to binary predictions at 0.5 cutoff\n",
    "- **Numerical stability**: Epsilon terms prevent division errors when precision/recall denominators are zero\n",
    "- **Complementary to mAP**: While mAP evaluates ranking, F1 scores assess classification accuracy at a specific threshold\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188d0bf",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer Configuration\n",
    "\n",
    "**Loss Function - BCEWithLogitsLoss**:\n",
    "- **Standard multi-label loss**: Combines sigmoid activation and binary cross-entropy in a numerically stable single operation\n",
    "- **Numerically stable**: Computes log-sum-exp trick internally to prevent overflow/underflow issues\n",
    "- **Multi-label formulation**: Treats each class independently, computing binary cross-entropy for all 80 classes simultaneously\n",
    "- **Probabilistic gradients**: Provides well-calibrated gradients for learning probability distributions, essential for multi-label classification\n",
    "\n",
    "**Optimizer - AdamW**:\n",
    "- **Adaptive learning rates**: Per-parameter learning rates automatically adjust based on first and second moment estimates of gradients\n",
    "- **Weight decay decoupling**: Applies L2 regularization correctly by decoupling it from gradient-based updates, fixing Adam's implementation flaw\n",
    "- **Fast convergence**: Adaptive method enables efficient convergence within 20 epochs on COCO dataset\n",
    "- **Regularization strength**: 5e-5 weight decay provides effective generalization without over-constraining the model\n",
    "\n",
    "**Learning Rate Scheduler - OneCycleLR**:\n",
    "- **Warmup phase**: Initial 30% of training (pct_start=0.3) gradually increases LR, allowing model to adapt to COCO data before aggressive learning\n",
    "- **Peak learning**: Reaches maximum LR (4e-4) for efficient feature learning in middle epochs\n",
    "- **Annealing phase**: Final 70% gradually decreases LR for fine-grained optimization and stable convergence\n",
    "- **Fast training**: OneCycleLR strategy enables strong performance within limited epoch budget (20 epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7416a826-17e6-40ae-9acf-c2e4e8b6ffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss criterion initialized: BCEWithLogitsLoss\n",
      "Optimizer and scheduler initialized:\n",
      "  - Optimizer: AdamW\n",
      "  - Learning rate: 0.0003\n",
      "  - Weight decay: 5e-05\n",
      "  - Scheduler: CosineAnnealingLR\n"
     ]
    }
   ],
   "source": [
    "# instantiation of loss criterion\n",
    "# instantiation of optimizer, registration of network parameters\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.L1Loss()\n",
    "\n",
    "print(\"Loss criterion initialized: BCEWithLogitsLoss\")\n",
    "#print(\"Loss criterion initialized: L1Loss\")\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "'''\n",
    "optimizer = Lion(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,\n",
    "    weight_decay=1e-2\n",
    ")\n",
    "'''\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LEARNING_RATE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.15\n",
    ")\n",
    "\n",
    "'''\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=NUM_EPOCHS, \n",
    "    eta_min=1e-6\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Optimizer and scheduler initialized:\")\n",
    "print(f\"  - Optimizer: AdamW\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  - Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c5e9639-803b-4050-8f93-d1270256e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"runs/coco_multi_label_shuffle\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "#writer = SummaryWriter(log_dir)\n",
    "\n",
    "#print(f\"Logs will be saved to: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd903e",
   "metadata": {},
   "source": [
    "## Validation Loop with Comprehensive Metrics\n",
    "\n",
    "**Evaluation Protocol**:\n",
    "1. **Evaluation mode**: `model.eval()` disables dropout and uses batch normalization running statistics\n",
    "2. **No gradients**: `torch.no_grad()` reduces memory consumption by not storing intermediate activations\n",
    "3. **Sigmoid activation**: Converts logits to [0,1] probabilities for multi-label prediction\n",
    "4. **Batch accumulation**: Collects predictions and labels in memory for metric computation\n",
    "\n",
    "**Comprehensive Metrics**:\n",
    "- **Loss**: Validation set loss for monitoring overfitting\n",
    "- **Exact match accuracy**: Percentage of samples with all 80 labels predicted correctly (very strict)\n",
    "- **Sample accuracy**: Average per-sample label accuracy (more lenient)\n",
    "- **Micro/Macro F1**: Complementary perspectives on classification performance\n",
    "- **mAP**: Primary ranking metric for model selection\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7571322-c64f-4661-b12a-2dc80d37969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graphs(summary_writer, epoch, train_results, val_results,\n",
    "                  train_class_results=None, val_class_results=None, \n",
    "                  class_names=None, mbatch_group=-1, mbatch_count=0, mbatch_losses=None):\n",
    "    \n",
    "    # Log mini-batch losses if available\n",
    "    if mbatch_group > 0 and mbatch_losses:\n",
    "        for i in range(len(mbatch_losses)):\n",
    "            summary_writer.add_scalar(\"Losses/Train mini-batches\",\n",
    "                                  mbatch_losses[i],\n",
    "                                  epoch * mbatch_count + (i+1)*mbatch_group)\n",
    "\n",
    "    # Log training vs validation losses\n",
    "    summary_writer.add_scalars(\"Losses/Train Loss vs Validation Loss\",\n",
    "                               {\"Train Loss\": train_results[\"loss\"],\n",
    "                                \"Validation Loss\": val_results[\"loss\"]},\n",
    "                               epoch + 1)\n",
    "\n",
    "    # Log F1 scores\n",
    "    summary_writer.add_scalars(\"Metrics/F1 Scores\",\n",
    "                               {\"Train Micro F1\": train_results[\"micro_f1\"],\n",
    "                                \"Validation Micro F1\": val_results[\"micro_f1\"],\n",
    "                                \"Train Macro F1\": train_results[\"macro_f1\"],\n",
    "                                \"Validation Macro F1\": val_results[\"macro_f1\"]},\n",
    "                               epoch + 1)\n",
    "\n",
    "    # Log accuracies\n",
    "    summary_writer.add_scalars(\"Metrics/Accuracies\",\n",
    "                               {\"Train Sample Accuracy\": train_results[\"sample_accuracy\"],\n",
    "                                \"Validation Sample Accuracy\": val_results[\"sample_accuracy\"],\n",
    "                                \"Train Exact Match\": train_results[\"exact_match_accuracy\"],\n",
    "                                \"Validation Exact Match\": val_results[\"exact_match_accuracy\"]},\n",
    "                               epoch + 1)\n",
    "\n",
    "    # Log learning rate\n",
    "    summary_writer.add_scalar(\"Learning Rate\", \n",
    "                             optimizer.param_groups[0]['lr'], \n",
    "                             epoch + 1)\n",
    "\n",
    "    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5e8528",
   "metadata": {},
   "source": [
    "## TensorBoard Logging Function\n",
    "\n",
    "**Logged Components**:\n",
    "1. **Mini-batch losses**: Track training loss at finer granularity for detailed convergence monitoring\n",
    "2. **Train vs Validation loss**: Side-by-side comparison to monitor generalization and detect overfitting\n",
    "3. **F1 scores**: Both micro/macro variants for train and validation sets\n",
    "4. **Accuracy metrics**: Exact match and sample-level accuracy for comprehensive evaluation\n",
    "5. **Learning rate**: Track scheduler's LR progression over epochs\n",
    "\n",
    "**Visualization Strategy**:\n",
    "- **Grouped scalars**: Related metrics plotted together (e.g., train/val losses on same graph) for easy comparison\n",
    "- **Flush operation**: Ensures data is written to disk immediately for real-time monitoring during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a081cd5",
   "metadata": {},
   "source": [
    "## Main Training Loop\n",
    "\n",
    "**Model Selection Strategy**:\n",
    "- **Metric-based checkpointing**: Saves best model according to `METRIC_OPTION` (currently validation loss for stable selection)\n",
    "- **Comprehensive checkpoint**: Stores model weights, optimizer state, scheduler state, and all best metrics for complete reproducibility\n",
    "\n",
    "**Windows Compatibility**: `if __name__ == '__main__' or 'ipykernel' in sys.modules` guards multiprocessing calls for proper Jupyter environment execution on Windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "402fb63f-81f2-4fd0-8756-1e039756cfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1f5373cfb248ceb7defe9d6d0d224d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6247c2c8b14645c0acd67865ab76388a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8490326b3eeb4052b9b98cb241d6bc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5258\n",
      "Validation Loss: 0.2401\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2772\n",
      "Macro F1: 0.0088\n",
      "mAP: 0.0471\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.2401)\n",
      "\n",
      "Epoch 2/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964a4cc018f2435a851390489e7aedbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50923c31b5de43d5a105c4a7f1668f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1654\n",
      "Validation Loss: 0.1379\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2772\n",
      "Macro F1: 0.0088\n",
      "mAP: 0.0651\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1379)\n",
      "\n",
      "Epoch 3/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc201c57135478684bd9ac463b8a76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649a1915ebec4fd99fc8ed861759ea21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1367\n",
      "Validation Loss: 0.1346\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2772\n",
      "Macro F1: 0.0088\n",
      "mAP: 0.0737\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1346)\n",
      "\n",
      "Epoch 4/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af841f91c464e3084357eba136d364d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f72e6c43646428cb7b757cc04bc6472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1349\n",
      "Validation Loss: 0.1338\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2771\n",
      "Macro F1: 0.0088\n",
      "mAP: 0.0793\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1338)\n",
      "\n",
      "Epoch 5/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c1ed3ef2bb4eeea05ffa68f0ce9cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea3fee4fd064b56b520e164cb17abe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1341\n",
      "Validation Loss: 0.1331\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2769\n",
      "Macro F1: 0.0090\n",
      "mAP: 0.0829\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1331)\n",
      "\n",
      "Epoch 6/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b91a096b40f4f678fa61ef684867afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14725b17701144449a76c276283327cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1334\n",
      "Validation Loss: 0.1322\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2642\n",
      "Macro F1: 0.0094\n",
      "mAP: 0.0860\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1322)\n",
      "\n",
      "Epoch 7/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2838031814964a40a64b9cdbbf4e9aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d06015f81d4fc6b92f3662f48b000e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1320\n",
      "Validation Loss: 0.1301\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2542\n",
      "Macro F1: 0.0098\n",
      "mAP: 0.0907\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1301)\n",
      "\n",
      "Epoch 8/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a557da1f44441a89ce62a44ed5fddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9899b32fe2214e6581636f5713468cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1292\n",
      "Validation Loss: 0.1262\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2513\n",
      "Macro F1: 0.0101\n",
      "mAP: 0.0993\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1262)\n",
      "\n",
      "Epoch 9/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ba781f43614972a9b61e68f052f07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc1b1ee269140c0be797bba4d69c183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1244\n",
      "Validation Loss: 0.1202\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2522\n",
      "Macro F1: 0.0103\n",
      "mAP: 0.1142\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1202)\n",
      "\n",
      "Epoch 10/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f0bbdd9a3644a7bfd1fbad4218ade5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532b1bfd4f614a9e816cd8bff6c404ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1184\n",
      "Validation Loss: 0.1141\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2696\n",
      "Macro F1: 0.0141\n",
      "mAP: 0.1361\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1141)\n",
      "\n",
      "Epoch 11/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ca419db39440f28cd89edaef58222f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594f9cc5762c4ddba0ea4c0e3c4ea295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1134\n",
      "Validation Loss: 0.1094\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2746\n",
      "Macro F1: 0.0155\n",
      "mAP: 0.1621\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1094)\n",
      "\n",
      "Epoch 12/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a7a7317a3e4246875bb3e69f673144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b04a2ade3c498d993d464f3a576945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1092\n",
      "Validation Loss: 0.1052\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2867\n",
      "Macro F1: 0.0196\n",
      "mAP: 0.1892\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1052)\n",
      "\n",
      "Epoch 13/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fb94596ea34e50a8684d8b669d67b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7645d07eae874e2e904707f708960e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1055\n",
      "Validation Loss: 0.1017\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2997\n",
      "Macro F1: 0.0248\n",
      "mAP: 0.2118\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.1017)\n",
      "\n",
      "Epoch 14/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6072443c6d3544ffb84a3ad01a476c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531343df3d49439ca18e967dabc654d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1028\n",
      "Validation Loss: 0.0994\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2989\n",
      "Macro F1: 0.0268\n",
      "mAP: 0.2267\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.0994)\n",
      "\n",
      "Epoch 15/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07279b6932d44d659d2a280efab86152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acb2200a2894aa6927424cae52107e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1008\n",
      "Validation Loss: 0.0975\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.3156\n",
      "Macro F1: 0.0348\n",
      "mAP: 0.2420\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.0975)\n",
      "\n",
      "Epoch 16/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375343646b964660a5edd281e6158342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b626056b8114292aa93af1c7416e973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0992\n",
      "Validation Loss: 0.0959\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.3244\n",
      "Macro F1: 0.0430\n",
      "mAP: 0.2561\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.0959)\n",
      "\n",
      "Epoch 17/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e69ee221ca41a3920e5da82935ee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf0da76d30d49d0b4da31881d5e5586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0978\n",
      "Validation Loss: 0.0946\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.3245\n",
      "Macro F1: 0.0439\n",
      "mAP: 0.2673\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.0946)\n",
      "\n",
      "Epoch 18/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e75e360c274a4c8294b441a9b8954a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfad3fc60f74305a312e41f99b96628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0965\n",
      "Validation Loss: 0.0933\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.3314\n",
      "Macro F1: 0.0504\n",
      "mAP: 0.2812\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.0933)\n",
      "\n",
      "Epoch 19/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a233d614c71941d6a91b989cf8b9fcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f367f04a4e4aa2b7e3e48a32b67a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0952\n",
      "Validation Loss: 0.0922\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.3406\n",
      "Macro F1: 0.0593\n",
      "mAP: 0.2928\n",
      "Current learning rate: 1.60e-05\n",
      "New best model saved (Validation Loss: 0.0922)\n",
      "\n",
      "Epoch 20/20\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dbf0c321ad4f47b246f5d1b15ae8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e049a58cdd40369ceca6c29fc37f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0941\n",
      "Validation Loss: 0.0911\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.3467\n",
      "Macro F1: 0.0708\n",
      "mAP: 0.3053\n",
      "Current learning rate: 1.61e-05\n",
      "New best model saved (Validation Loss: 0.0911)\n",
      "\n",
      "Training completed!\n",
      "Best model saved to: best_coco_shuffle_model.pth\n"
     ]
    }
   ],
   "source": [
    "# for multiprocessing in windows+jupyter, it's better to put the training process in '__main__' for avoiding pickle problem\n",
    "if __name__ == '__main__' or 'ipykernel' in sys.modules: \n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_micro_f1 = 0.0\n",
    "    best_val_macro_f1 = 0.0\n",
    "    best_val_mAP = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        train_loss = train_loop(train_loader, model, criterion, optimizer, device)\n",
    "\n",
    "        #train_results = validation_loop(train_loader, model, criterion, device)\n",
    "        #train_results['loss'] = train_loss  \n",
    "        \n",
    "        val_results = validation_loop(val_loader, model, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_results['loss']:.4f}\")\n",
    "        print(f\"Exact Match Accuracy: {val_results['exact_match_accuracy']:.4f}\")\n",
    "        print(f\"Sample Accuracy: {val_results['sample_accuracy']:.4f}\")\n",
    "        print(f\"Micro F1: {val_results['micro_f1']:.4f}\")\n",
    "        print(f\"Macro F1: {val_results['macro_f1']:.4f}\")\n",
    "        print(f\"mAP: {val_results['mAP']:.4f}\")\n",
    "        print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        #update_graphs(writer, epoch, train_results, val_results)\n",
    "        \n",
    "        # Model selection based on METRIC_OPTION\n",
    "        save_model = False\n",
    "        metric_name = \"\"\n",
    "        metric_value = 0.0\n",
    "        \n",
    "        if METRIC_OPTION == 'val_loss':\n",
    "            if val_results['loss'] < best_val_loss:\n",
    "                best_val_loss = val_results['loss']\n",
    "                save_model = True\n",
    "                metric_name = \"Validation Loss\"\n",
    "                metric_value = best_val_loss\n",
    "                \n",
    "        elif METRIC_OPTION == 'micro_f1':\n",
    "            if val_results['micro_f1'] > best_val_micro_f1:\n",
    "                best_val_micro_f1 = val_results['micro_f1']\n",
    "                save_model = True\n",
    "                metric_name = \"Micro F1\"\n",
    "                metric_value = best_val_micro_f1\n",
    "                \n",
    "        elif METRIC_OPTION == 'macro_f1':\n",
    "            if val_results['macro_f1'] > best_val_macro_f1:\n",
    "                best_val_macro_f1 = val_results['macro_f1']\n",
    "                save_model = True\n",
    "                metric_name = \"Macro F1\"\n",
    "                metric_value = best_val_macro_f1\n",
    "                \n",
    "        elif METRIC_OPTION == 'mAP':\n",
    "            if val_results['mAP'] > best_val_mAP:\n",
    "                best_val_mAP = val_results['mAP']\n",
    "                save_model = True\n",
    "                metric_name = \"mAP\"\n",
    "                metric_value = best_val_mAP\n",
    "        \n",
    "        # Save model if a new best metric was achieved\n",
    "        if save_model:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_micro_f1': best_val_micro_f1,\n",
    "                'best_val_macro_f1': best_val_macro_f1,\n",
    "                'best_val_mAP': best_val_mAP,\n",
    "                'train_loss': train_loss,\n",
    "                'val_results': val_results,\n",
    "                'metric_option': METRIC_OPTION,\n",
    "            }, MODEL_SAVE_PATH)\n",
    "            print(f\"New best model saved ({metric_name}: {metric_value:.4f})\")\n",
    "        \n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best model saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    #writer.close()\n",
    "    #print(\"TensorBoard writer closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8798da6-a130-4304-ad9e-1eba45f82820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting test prediction program\n",
      "============================================================\n",
      "Test inference hyperparameters:\n",
      "  - Test batch size: 64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Starting test prediction program\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "BATCH_SIZE_TEST = 64\n",
    "\n",
    "print(f\"Test inference hyperparameters:\")\n",
    "print(f\"  - Test batch size: {BATCH_SIZE_TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa2be1e9-1d1a-4b6c-9d74-50d5d1c9e7ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test directories and files:\n",
      "  - Test images: ms-coco\\images\\test-resized\\test-resized\n",
      "  - Trained model: best_coco_shuffle_model.pth\n",
      "  - Output JSON: coco_predictions_shuffle_v9.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test directories and files:\")\n",
    "print(f\"  - Test images: {TEST_IMG_DIR}\")\n",
    "print(f\"  - Trained model: {MODEL_SAVE_PATH}\")\n",
    "print(f\"  - Output JSON: {OUTPUT_JSON_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfdace-c02d-48fb-8c19-f4a2ba0cef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = COCOTestImageDataset(\n",
    "    img_dir=TEST_IMG_DIR,\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    shuffle=False,  # No shuffling needed for testing\n",
    "    num_workers=4,  \n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Test batch count: {len(test_loader)}\")\n",
    "print(f\"Test loader using {test_loader.num_workers} workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cdcde7f-2d97-4e31-94bd-2e848e13ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unicorn\\AppData\\Local\\Temp\\ipykernel_4560\\3238392438.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model weights from: best_coco_shuffle_model.pth\n",
      "Model training epoch: 20\n",
      "Best validation loss: 0.0000\n",
      "Model ready for inference\n"
     ]
    }
   ],
   "source": [
    "test_model = COCOMultiLabelClassifier(num_classes=NUM_CLASSES, pretrained=False)\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Successfully loaded model weights from: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"Model training epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"Best validation loss: {checkpoint['best_val_micro_f1']:.4f}\")\n",
    "else:\n",
    "    print(f\"Trained model file not found: {MODEL_SAVE_PATH}\")\n",
    "    print(\"Please run the training program first\")\n",
    "    raise FileNotFoundError(f\"Model file not found: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "test_model = test_model.to(device)\n",
    "test_model.eval()\n",
    "print(\"Model ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74382136",
   "metadata": {},
   "source": [
    "## Test Paths Verification\n",
    "\n",
    "**Output Components**:\n",
    "- **Test images**: 4952 unlabeled images for challenge submission\n",
    "- **Model checkpoint**: Best model saved during training (based on mAP metric)\n",
    "- **Predictions JSON**: Output file containing predicted class indices per image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2115528b-74b2-4301-b0e2-6702d872cd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dictionary initialized\n"
     ]
    }
   ],
   "source": [
    "predictions_dict = {}\n",
    "print(\"Output dictionary initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83162c2-b3da-4812-a23a-2842f33cf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting prediction loop...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, filenames) in enumerate(tqdm(test_loader, desc=\"Predicting\")):\n",
    "        # Get mini-batch\n",
    "        images = images.to(device)\n",
    "        \n",
    "        outputs = test_model(images)\n",
    "        \n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predictions = (probabilities > THRESHOLD).cpu().numpy()\n",
    "        \n",
    "        # Update dictionary entries, write corresponding class indices\n",
    "        for i, filename in enumerate(filenames):\n",
    "            predicted_classes = []\n",
    "            for class_idx in range(NUM_CLASSES):\n",
    "                if predictions[i, class_idx]:\n",
    "                    predicted_classes.append(class_idx)\n",
    "            \n",
    "            predictions_dict[filename] = predicted_classes\n",
    "\n",
    "print(f\"Prediction completed, processed {len(predictions_dict)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9b418",
   "metadata": {},
   "source": [
    "## Model Loading for Inference\n",
    "\n",
    "**Loading Process**:\n",
    "1. **Initialize architecture**: Create model with random weights (`pretrained=False` to avoid downloading ImageNet weights)\n",
    "2. **Load checkpoint**: Retrieve saved state dictionary from training\n",
    "3. **Restore weights**: Apply trained parameters to model\n",
    "4. **Device placement**: Move to GPU for accelerated inference\n",
    "5. **Evaluation mode**: Disable dropout and batch normalization training behavior\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7fa075e-fd5d-4750-87f0-9d458b7f2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prediction results to: coco_predictions_shuffle_v8.json\n",
      "  Sample 000000000139: predicted classes [56, 57, 58, 60, 62]\n",
      "  Sample 000000000285: predicted classes []\n",
      "  Sample 000000000632: predicted classes [56, 57]\n",
      "  Sample 000000000724: predicted classes [11]\n",
      "  Sample 000000000776: predicted classes [77]\n",
      "JSON file successfully saved to: coco_predictions_shuffle_v8.json\n",
      "File size: 200.17 KB\n",
      "============================================================\n",
      "Test prediction program completed!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving prediction results to: {OUTPUT_JSON_FILE}\")\n",
    "\n",
    "# Show some sample predictions\n",
    "sample_count = 0\n",
    "for filename, predicted_classes in predictions_dict.items():\n",
    "    if sample_count < 5:  # Show only first 5 samples\n",
    "        print(f\"  Sample {filename}: predicted classes {predicted_classes}\")\n",
    "        sample_count += 1\n",
    "\n",
    "try:\n",
    "    with open(OUTPUT_JSON_FILE, 'w') as f:\n",
    "        json.dump(predictions_dict, f, indent=2)\n",
    "    print(f\"JSON file successfully saved to: {OUTPUT_JSON_FILE}\")\n",
    "    \n",
    "    # Check file size\n",
    "    file_size = os.path.getsize(OUTPUT_JSON_FILE)\n",
    "    print(f\"File size: {file_size / 1024:.2f} KB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Test prediction program completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f63168",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
