{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9094f777-f1c1-4224-b248-3a044861fca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# import statements for python, torch and companion libraries and your own modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import shufflenet_v2_x1_0, ShuffleNet_V2_X1_0_Weights\n",
    "from PIL import Image\n",
    "\n",
    "# Import dataset classes from dataset.py for Windows multiprocessing support\n",
    "from dataset import COCOTrainImageDataset, COCOTestImageDataset, ValidationDataset\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7286f114-09ea-4ea1-88d3-362671cb0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seed to ensure reproducibility of results\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = False  \n",
    "    torch.backends.cudnn.benchmark = True  \n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c0880e-608e-4749-8387-ae38063c077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global variables and hyperparameters defined:\n",
      "  - Batch size: 64\n",
      "  - Number of epochs: 15\n",
      "  - Learning rate: 0.0001\n",
      "  - Validation split: 0.1\n",
      "  - Threshold: 0.5\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# global variables defining training hyper-parameters among other things \n",
    "BATCH_SIZE = 64  \n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4  \n",
    "WEIGHT_DECAY = 1e-5\n",
    "NUM_CLASSES = 80\n",
    "VALIDATION_SPLIT = 0.1\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "print(\"Global variables and hyperparameters defined:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Number of epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Validation split: {VALIDATION_SPLIT}\")\n",
    "print(f\"  - Threshold: {THRESHOLD}\")\n",
    "\n",
    "# device initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b31cd1d-5acf-4cd8-8dde-49409a28d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directories initialization\n",
    "DATA_DIR = \"ms-coco\"\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"images\", \"train-resized\", \"train-resized\")\n",
    "TEST_IMG_DIR = os.path.join(DATA_DIR, \"images\", \"test-resized\", \"test-resized\")\n",
    "TRAIN_LABELS_DIR = os.path.join(DATA_DIR, \"labels\", \"train\")\n",
    "MODEL_SAVE_PATH = \"best_coco_shuffle_model.pth\"\n",
    "OUTPUT_JSON_FILE = \"coco_predictions_shuffle_v3.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64c5467-88d4-4ab5-b910-9a8d9650fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class definitions\n",
    "classes = (\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \n",
    "           \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
    "           \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",       \n",
    "           \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "           \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "           \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n",
    "           \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \n",
    "           \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \n",
    "           \"hair drier\", \"toothbrush\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377a65a4-12c8-4855-bc63-32c78e291553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories and class names defined:\n",
      "  - Training images: ms-coco\\images\\train-resized\\train-resized\n",
      "  - Test images: ms-coco\\images\\test-resized\\test-resized\n",
      "  - Training labels: ms-coco\\labels\\train\n",
      "  - Dataset contains 80 classes\n"
     ]
    }
   ],
   "source": [
    "print(\"Data directories and class names defined:\")\n",
    "print(f\"  - Training images: {TRAIN_IMG_DIR}\")\n",
    "print(f\"  - Test images: {TEST_IMG_DIR}\")\n",
    "print(f\"  - Training labels: {TRAIN_LABELS_DIR}\")\n",
    "print(f\"  - Dataset contains {NUM_CLASSES} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "743d1e05-9fe4-4d16-9c3f-eb43e891d062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCOTrainImageDataset imported from dataset.py\n"
     ]
    }
   ],
   "source": [
    "# COCOTrainImageDataset class has been moved to dataset.py for Windows multiprocessing support\n",
    "print(\"COCOTrainImageDataset imported from dataset.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd008a9c-c6d3-4363-bb58-7b8bf401e598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCOTestImageDataset imported from dataset.py\n"
     ]
    }
   ],
   "source": [
    "# COCOTestImageDataset class has been moved to dataset.py for Windows multiprocessing support\n",
    "print(\"COCOTestImageDataset imported from dataset.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca2d4de6-7658-42e9-bdc3-6d27c95aadc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Full training dataset size: 65000\n",
      "Training set size: 58500\n",
      "✅ Validation set size: 6500\n"
     ]
    }
   ],
   "source": [
    "# instantiation of transforms, datasets and data loaders\n",
    "# TIP : use torch.utils.data.random_split to split the training set into train and validation subsets\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR), \n",
    "    transforms.RandomHorizontalFlip(p=0.5),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create full training dataset\n",
    "print(\"Loading dataset...\")\n",
    "full_train_dataset = COCOTrainImageDataset(\n",
    "    img_dir=TRAIN_IMG_DIR,\n",
    "    annotations_dir=TRAIN_LABELS_DIR,\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "print(f\"Full training dataset size: {len(full_train_dataset)}\")\n",
    "\n",
    "# Split training data into train and validation subsets using torch.utils.data.random_split\n",
    "train_size = int((1 - VALIDATION_SPLIT) * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"✅ Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eee4efe-304a-4564-b4d1-f424d704ba64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValidationDataset imported from dataset.py\n"
     ]
    }
   ],
   "source": [
    "# ValidationDataset class has been moved to dataset.py for Windows multiprocessing support\n",
    "print(\"ValidationDataset imported from dataset.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59c0b5c2-fe6a-4ca1-9973-bbc1d18351cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created successfully with Windows multiprocessing support\n",
      "  - Training loader: 914 batches, 4 workers\n",
      "  - Validation loader: 102 batches, 4 workers\n"
     ]
    }
   ],
   "source": [
    "val_dataset_transformed = ValidationDataset(val_dataset, val_transforms)\n",
    "\n",
    "# Create data loaders with Windows-compatible multiprocessing settings\n",
    "# For Windows, we can now use num_workers > 0 since dataset classes are in separate .py file\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=4,  \n",
    "    pin_memory=True,  \n",
    "    drop_last=True,\n",
    "    persistent_workers=True  # Keep workers alive between epochs\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_transformed, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4,  \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(\"Data loaders created successfully with Windows multiprocessing support\")\n",
    "print(f\"  - Training loader: {len(train_loader)} batches, {train_loader.num_workers} workers\")\n",
    "print(f\"  - Validation loader: {len(val_loader)} batches, {val_loader.num_workers} workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c22b8d27-dafe-4770-8c34-603330f9df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOMultiLabelClassifier(nn.Module):\n",
    "    \"\"\"Multi-label classifier based on ShuffleNet V2 x1.0\"\"\"\n",
    "    def __init__(self, num_classes: int = 80, pretrained: bool = True):\n",
    "        super(COCOMultiLabelClassifier, self).__init__()\n",
    "        \n",
    "        # Use pre-trained ShuffleNet V2 x1.0 as backbone\n",
    "        if pretrained:\n",
    "            self.backbone = shufflenet_v2_x1_0(weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            self.backbone = shufflenet_v2_x1_0(weights=None)\n",
    "        \n",
    "        # Get feature dimension (ShuffleNet V2 x1.0 has 1024 output features)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Replace classification head with multi-label classification head\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d09d758-7c30-4df4-a2b4-e77c429d372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Model loaded to device: cuda\n",
      "  - Total parameters: 1,819,444\n",
      "  - Trainable parameters: 1,819,444\n"
     ]
    }
   ],
   "source": [
    "# instantiation and preparation of network model\n",
    "print(\"Initializing model...\")\n",
    "model = COCOMultiLabelClassifier(num_classes=NUM_CLASSES, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded to device: {device}\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e22549af-61ff-4797-9526-0e8851ba54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for select the best model\n",
    "def calculate_f1_metrics(predictions, labels, threshold=0.5):\n",
    "\n",
    "    predictions_binary = (predictions > threshold).float()\n",
    "    tp = (predictions_binary * labels).sum()\n",
    "    fp = (predictions_binary * (1 - labels)).sum() \n",
    "    fn = ((1 - predictions_binary) * labels).sum()\n",
    "    \n",
    "    micro_precision = tp / (tp + fp + 1e-8)\n",
    "    micro_recall = tp / (tp + fn + 1e-8)\n",
    "    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall + 1e-8)\n",
    "    \n",
    "    class_f1s = []\n",
    "    for c in range(labels.shape[1]):\n",
    "        tp_c = (predictions_binary[:, c] * labels[:, c]).sum()\n",
    "        fp_c = (predictions_binary[:, c] * (1 - labels[:, c])).sum()\n",
    "        fn_c = ((1 - predictions_binary[:, c]) * labels[:, c]).sum()\n",
    "        \n",
    "        prec_c = tp_c / (tp_c + fp_c + 1e-8)\n",
    "        rec_c = tp_c / (tp_c + fn_c + 1e-8)\n",
    "        f1_c = 2 * prec_c * rec_c / (prec_c + rec_c + 1e-8)\n",
    "        class_f1s.append(f1_c)\n",
    "    \n",
    "    macro_f1 = torch.stack(class_f1s).mean()\n",
    "    return float(micro_f1), float(macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1bf3d5-fb71-4fb7-8610-8167335f49de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader: DataLoader, net: nn.Module, criterion: nn.Module, \n",
    "               optimizer: optim.Optimizer, device: torch.device) -> float:\n",
    "\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\",position=0, leave=True):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "566f95e5-0e55-482d-91a7-2c97680cf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(val_loader: DataLoader, net: nn.Module, criterion: nn.Module, \n",
    "                   device: torch.device) -> Dict[str, float]:\n",
    "\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\",position=0, leave=True):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            batch_loss = criterion(outputs, labels)\n",
    "            val_loss += batch_loss.item() * images.size(0)\n",
    "            \n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            \n",
    "            all_predictions.append(probabilities.cpu())  # save the probabilities instead of predictions\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    micro_f1, macro_f1 = calculate_f1_metrics(all_predictions, all_labels)\n",
    "\n",
    "    predictions_binary = (all_predictions > THRESHOLD).float()\n",
    "    exact_match = (all_predictions == all_labels).all(dim=1).float().mean().item()\n",
    "    \n",
    "    sample_accuracy = ((all_predictions == all_labels).float().mean(dim=1)).mean().item()\n",
    "    \n",
    "    return {\n",
    "        'loss': val_loss,\n",
    "        'exact_match_accuracy': exact_match,\n",
    "        'sample_accuracy': sample_accuracy,\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7416a826-17e6-40ae-9acf-c2e4e8b6ffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss criterion initialized: BCEWithLogitsLoss\n",
      "Optimizer and scheduler initialized:\n",
      "  - Optimizer: AdamW\n",
      "  - Learning rate: 0.0001\n",
      "  - Weight decay: 1e-05\n",
      "  - Scheduler: CosineAnnealingLR\n"
     ]
    }
   ],
   "source": [
    "# instantiation of loss criterion\n",
    "# instantiation of optimizer, registration of network parameters\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(\"Loss criterion initialized: BCEWithLogitsLoss\")\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=NUM_EPOCHS, \n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(\"Optimizer and scheduler initialized:\")\n",
    "print(f\"  - Optimizer: AdamW\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  - Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c5e9639-803b-4050-8f93-d1270256e3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs will be saved to: runs/coco_multi_label_shuffle\n",
      "   To view logs, run: tensorboard --logdir=runs\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"runs/coco_multi_label_shuffle\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Logs will be saved to: {log_dir}\")\n",
    "print(\"   To view logs, run: tensorboard --logdir=runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "402fb63f-81f2-4fd0-8756-1e039756cfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c6f343c358417e97f0eb9c33724648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21b414316a44bff9d28058b56c96df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b184f241e0414c9615f638c4100e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1687\n",
      "Validation Loss: 0.1089\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.2811\n",
      "Macro F1: 0.0148\n",
      "Current learning rate: 9.89e-05\n",
      "New best model saved (Macro F1: 0.0148)\n",
      "\n",
      "Epoch 2/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab3871bb4e141d1912dbb7e0aac4b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8612f6cc00b54cd5997829009623735c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1011\n",
      "Validation Loss: 0.0921\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.3454\n",
      "Macro F1: 0.0590\n",
      "Current learning rate: 9.57e-05\n",
      "New best model saved (Macro F1: 0.0590)\n",
      "\n",
      "Epoch 3/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424c758b24724540a52bcb3e6905e284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2edf3f6b254e76ac41e164407e4ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0907\n",
      "Validation Loss: 0.0842\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.4287\n",
      "Macro F1: 0.1680\n",
      "Current learning rate: 9.05e-05\n",
      "New best model saved (Macro F1: 0.1680)\n",
      "\n",
      "Epoch 4/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec904774aa04a948e3758137ba838dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94de611047f94380ba06845adf50d217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0837\n",
      "Validation Loss: 0.0786\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.4900\n",
      "Macro F1: 0.2555\n",
      "Current learning rate: 8.36e-05\n",
      "New best model saved (Macro F1: 0.2555)\n",
      "\n",
      "Epoch 5/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7eabebe9374faf8bf9c9363009d640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6db3b5080aa4ee6ad79440ff0584c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0785\n",
      "Validation Loss: 0.0752\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5255\n",
      "Macro F1: 0.3113\n",
      "Current learning rate: 7.52e-05\n",
      "New best model saved (Macro F1: 0.3113)\n",
      "\n",
      "Epoch 6/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad580997222140f9a11136bd8d83dfe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dff401caf24817bd38412d32efd48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0748\n",
      "Validation Loss: 0.0727\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5418\n",
      "Macro F1: 0.3536\n",
      "Current learning rate: 6.58e-05\n",
      "New best model saved (Macro F1: 0.3536)\n",
      "\n",
      "Epoch 7/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15f552154cc4698b637b84251704c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a94ac9bcc94951aa473879e6d2a1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0722\n",
      "Validation Loss: 0.0711\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5575\n",
      "Macro F1: 0.3791\n",
      "Current learning rate: 5.57e-05\n",
      "New best model saved (Macro F1: 0.3791)\n",
      "\n",
      "Epoch 8/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa6c15e583d4d54843bd903ef1b4129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e1bb215fc84d1ab7713b675291e31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0701\n",
      "Validation Loss: 0.0701\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5769\n",
      "Macro F1: 0.4138\n",
      "Current learning rate: 4.53e-05\n",
      "New best model saved (Macro F1: 0.4138)\n",
      "\n",
      "Epoch 9/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39063414f644f02aa59178646d48df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6df41f1119045fcac4e763851d8611f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0685\n",
      "Validation Loss: 0.0692\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5854\n",
      "Macro F1: 0.4280\n",
      "Current learning rate: 3.52e-05\n",
      "New best model saved (Macro F1: 0.4280)\n",
      "\n",
      "Epoch 10/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8d47e7c87e41e5b978cfe27219f716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e89137d5dcb437d9ad934d58a475280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0673\n",
      "Validation Loss: 0.0688\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5867\n",
      "Macro F1: 0.4354\n",
      "Current learning rate: 2.58e-05\n",
      "New best model saved (Macro F1: 0.4354)\n",
      "\n",
      "Epoch 11/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48267b0b19b4022823404b7dcf6c370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904dfded0ad44b8087c30424d2b4347d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0664\n",
      "Validation Loss: 0.0685\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5936\n",
      "Macro F1: 0.4436\n",
      "Current learning rate: 1.74e-05\n",
      "New best model saved (Macro F1: 0.4436)\n",
      "\n",
      "Epoch 12/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f6dd75912a468f9ffd3c614d390ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d7a819273c4e8e890eac20047df687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0658\n",
      "Validation Loss: 0.0683\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5943\n",
      "Macro F1: 0.4471\n",
      "Current learning rate: 1.05e-05\n",
      "New best model saved (Macro F1: 0.4471)\n",
      "\n",
      "Epoch 13/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c035fa2303e94a55aacffb627c096182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cffe21702040b5a0a7e6a509436eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0653\n",
      "Validation Loss: 0.0682\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5936\n",
      "Macro F1: 0.4492\n",
      "Current learning rate: 5.28e-06\n",
      "New best model saved (Macro F1: 0.4492)\n",
      "\n",
      "Epoch 14/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d29ac554d5f460bad5c01fc01f9a540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ef913c5e1b4006b70188e33836b598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0650\n",
      "Validation Loss: 0.0683\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5920\n",
      "Macro F1: 0.4437\n",
      "Current learning rate: 2.08e-06\n",
      "\n",
      "Epoch 15/15\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35794cdd9db24cd5ba5d2dae6d1d3377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05b342c639f46bca677863c360bb6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0648\n",
      "Validation Loss: 0.0681\n",
      "Exact Match Accuracy: 0.0000\n",
      "Sample Accuracy: 0.0000\n",
      "Micro F1: 0.5971\n",
      "Macro F1: 0.4529\n",
      "Current learning rate: 1.00e-06\n",
      "New best model saved (Macro F1: 0.4529)\n",
      "\n",
      "Training completed!\n",
      "Best model saved to: best_coco_shuffle_model.pth\n"
     ]
    }
   ],
   "source": [
    "# for multiprocessing in windows+jupyter, it's better to put the training process in '__main__' for avoiding pickle problem\n",
    "if __name__ == '__main__' or 'ipykernel' in sys.modules: \n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_micro_f1 = 0.0\n",
    "    best_val_macro_f1 = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        train_loss = train_loop(train_loader, model, criterion, optimizer, device)\n",
    "        \n",
    "        val_results = validation_loop(val_loader, model, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_results['loss']:.4f}\")\n",
    "        print(f\"Exact Match Accuracy: {val_results['exact_match_accuracy']:.4f}\")\n",
    "        print(f\"Sample Accuracy: {val_results['sample_accuracy']:.4f}\")\n",
    "        print(f\"Micro F1: {val_results['micro_f1']:.4f}\")\n",
    "        print(f\"Macro F1: {val_results['macro_f1']:.4f}\")\n",
    "        print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        # Model selection: save best model based on validation loss\n",
    "        '''\n",
    "        if val_results['loss'] < best_val_loss:\n",
    "            best_val_loss = val_results['loss']\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'train_loss': train_loss,\n",
    "                'val_results': val_results,\n",
    "            }, MODEL_SAVE_PATH)\n",
    "            print(f\"New best model saved (Validation Loss: {best_val_loss:.4f})\")\n",
    "        '''\n",
    "        '''\n",
    "        # Model selection: save best model based on micro f1\n",
    "        if val_results['micro_f1'] > best_val_micro_f1:\n",
    "            best_val_micro_f1 = val_results['micro_f1']\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_micro_f1': best_val_micro_f1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_results': val_results,\n",
    "            }, MODEL_SAVE_PATH)\n",
    "            print(f\"New best model saved (Micro F1: {best_val_micro_f1:.4f})\")\n",
    "        '''\n",
    "        \n",
    "        # Model selection: save best model based on macro f1\n",
    "        if val_results['macro_f1'] > best_val_macro_f1:\n",
    "            best_val_macro_f1 = val_results['macro_f1']\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_macro_f1': best_val_macro_f1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_results': val_results,\n",
    "            }, MODEL_SAVE_PATH)\n",
    "            print(f\"New best model saved (Macro F1: {best_val_macro_f1:.4f})\")\n",
    "        \n",
    "        \n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best model saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8798da6-a130-4304-ad9e-1eba45f82820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting test prediction program\n",
      "============================================================\n",
      "Test inference hyperparameters:\n",
      "  - Test batch size: 64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Starting test prediction program\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "BATCH_SIZE_TEST = 64\n",
    "\n",
    "print(f\"Test inference hyperparameters:\")\n",
    "print(f\"  - Test batch size: {BATCH_SIZE_TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa2be1e9-1d1a-4b6c-9d74-50d5d1c9e7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test directories and files:\n",
      "  - Test images: ms-coco\\images\\test-resized\\test-resized\n",
      "  - Trained model: best_coco_shuffle_model.pth\n",
      "  - Output JSON: coco_predictions_shuffle_v3.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test directories and files:\")\n",
    "print(f\"  - Test images: {TEST_IMG_DIR}\")\n",
    "print(f\"  - Trained model: {MODEL_SAVE_PATH}\")\n",
    "print(f\"  - Output JSON: {OUTPUT_JSON_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35bfdace-c02d-48fb-8c19-f4a2ba0cef3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 4952\n",
      "Test batch count: 78\n",
      "Test loader using 4 workers\n"
     ]
    }
   ],
   "source": [
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = COCOTestImageDataset(\n",
    "    img_dir=TEST_IMG_DIR,\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    shuffle=False,  # No shuffling needed for testing\n",
    "    num_workers=4,  \n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Test batch count: {len(test_loader)}\")\n",
    "print(f\"Test loader using {test_loader.num_workers} workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cdcde7f-2d97-4e31-94bd-2e848e13ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unicorn\\AppData\\Local\\Temp\\ipykernel_21520\\4159674509.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model weights from: best_coco_shuffle_model.pth\n",
      "Model training epoch: 15\n",
      "Best validation loss: 0.4529\n",
      "Model ready for inference\n"
     ]
    }
   ],
   "source": [
    "test_model = COCOMultiLabelClassifier(num_classes=NUM_CLASSES, pretrained=False)\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Successfully loaded model weights from: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"Model training epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"Best validation loss: {checkpoint['best_val_macro_f1']:.4f}\")\n",
    "else:\n",
    "    print(f\"Trained model file not found: {MODEL_SAVE_PATH}\")\n",
    "    print(\"Please run the training program first\")\n",
    "    raise FileNotFoundError(f\"Model file not found: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "test_model = test_model.to(device)\n",
    "test_model.eval()\n",
    "print(\"Model ready for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2115528b-74b2-4301-b0e2-6702d872cd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dictionary initialized\n"
     ]
    }
   ],
   "source": [
    "predictions_dict = {}\n",
    "print(\"Output dictionary initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b83162c2-b3da-4812-a23a-2842f33cf670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction loop...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0074b708094b7c843851296ee700fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed, processed 4952 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting prediction loop...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, filenames) in enumerate(tqdm(test_loader, desc=\"Predicting\")):\n",
    "        # Get mini-batch\n",
    "        images = images.to(device)\n",
    "        \n",
    "        outputs = test_model(images)\n",
    "        \n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predictions = (probabilities > THRESHOLD).cpu().numpy()\n",
    "        \n",
    "        # Update dictionary entries, write corresponding class indices\n",
    "        for i, filename in enumerate(filenames):\n",
    "            predicted_classes = []\n",
    "            for class_idx in range(NUM_CLASSES):\n",
    "                if predictions[i, class_idx]:\n",
    "                    predicted_classes.append(class_idx)\n",
    "            \n",
    "            predictions_dict[filename] = predicted_classes\n",
    "\n",
    "print(f\"Prediction completed, processed {len(predictions_dict)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7fa075e-fd5d-4750-87f0-9d458b7f2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prediction results to: coco_predictions_shuffle_v3.json\n",
      "  Sample 000000000139: predicted classes [56, 57, 58, 60, 62]\n",
      "  Sample 000000000285: predicted classes []\n",
      "  Sample 000000000632: predicted classes [56, 57, 58, 62, 73]\n",
      "  Sample 000000000724: predicted classes [11]\n",
      "  Sample 000000000776: predicted classes [77]\n",
      "JSON file successfully saved to: coco_predictions_shuffle_v3.json\n",
      "File size: 196.40 KB\n",
      "============================================================\n",
      "Test prediction program completed!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving prediction results to: {OUTPUT_JSON_FILE}\")\n",
    "\n",
    "# Show some sample predictions\n",
    "sample_count = 0\n",
    "for filename, predicted_classes in predictions_dict.items():\n",
    "    if sample_count < 5:  # Show only first 5 samples\n",
    "        print(f\"  Sample {filename}: predicted classes {predicted_classes}\")\n",
    "        sample_count += 1\n",
    "\n",
    "try:\n",
    "    with open(OUTPUT_JSON_FILE, 'w') as f:\n",
    "        json.dump(predictions_dict, f, indent=2)\n",
    "    print(f\"JSON file successfully saved to: {OUTPUT_JSON_FILE}\")\n",
    "    \n",
    "    # Check file size\n",
    "    file_size = os.path.getsize(OUTPUT_JSON_FILE)\n",
    "    print(f\"File size: {file_size / 1024:.2f} KB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Test prediction program completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d62efd-5178-40ec-89fd-4298955a7688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
